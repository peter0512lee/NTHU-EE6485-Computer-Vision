{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0MrEVoVmhOy"
      },
      "source": [
        "# Computer Vision Homework 3: Big vs Small Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0hnrUlYrGWS"
      },
      "source": [
        "## Brief"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_zuWZp5rSyN"
      },
      "source": [
        "Due date: Nov 16, 2022\n",
        "\n",
        "Required files: `homework-3.ipynb`, `report.pdf`\n",
        "\n",
        "To download the jupyter notebook from colab, you can refer to the colab tutorial we gave.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om7423NauKQ6"
      },
      "source": [
        "## Codes for Problem 1 and Problem 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX6pBqvV6RCq"
      },
      "source": [
        "### Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73wanLwflUdb"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
        "from torchvision import transforms, models, datasets\n",
        "from tqdm import tqdm\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtXEq_Yx5j-L"
      },
      "source": [
        "### Check GPU Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yz3wOsYwmEz8"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using {device} device')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbpaGDdwnX9g"
      },
      "outputs": [],
      "source": [
        "! nvidia-smi -L"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAoPtdOR5ojk"
      },
      "source": [
        "### Set the Seed to Reproduce the Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wphy638XBNj-"
      },
      "outputs": [],
      "source": [
        "def set_all_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "set_all_seed(123)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLmcH3NAH4wq"
      },
      "source": [
        "### Create Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VHp_O3_JgZE"
      },
      "outputs": [],
      "source": [
        "batch_size = 256\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Pad(4, padding_mode='reflect'),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='data', train=True, download=True, transform=train_transform)\n",
        "valid_dataset = datasets.CIFAR10(root='data', train=False, download=True, transform=test_transform)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
        "\n",
        "sixteenth_train_sampler = RandomSampler(train_dataset, num_samples=len(train_dataset)//16)\n",
        "half_train_sampler = RandomSampler(train_dataset, num_samples=len(train_dataset)//2)\n",
        "\n",
        "sixteenth_train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=sixteenth_train_sampler)\n",
        "half_train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=half_train_sampler)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjFDtcWRnFS9"
      },
      "source": [
        "### Load Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgZV0CodnFS9"
      },
      "outputs": [],
      "source": [
        "# HINT: Remember to change the model to 'resnet50' and the weights to weights=\"IMAGENET1K_V1\" when needed.\n",
        "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', weights=None)\n",
        "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', weights=None)\n",
        "# model = torch.hub.load('pytorch/vision:v0.10.0',\n",
        "#                        'resnet18', weights=\"IMAGENET1K_V1\")\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0',\n",
        "                       'resnet50', weights=\"IMAGENET1K_V1\")\n",
        "\n",
        "# Background: The original resnet18 is designed for ImageNet dataset to predict 1000 classes.\n",
        "# TODO: Change the output of the model to 10 class.\n",
        "model.fc = nn.Linear(model.fc.in_features, 10)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZo50knhnFS_"
      },
      "source": [
        "### Training and Testing Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlXKJeYWnFTA"
      },
      "outputs": [],
      "source": [
        "# TODO: Fill in the code cell according to the pytorch tutorial we gave.\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    num_batches = len(dataloader)\n",
        "    size = len(dataloader.dataset)\n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for X, y in tqdm(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        pred = pred.argmax(dim=1, keepdim=True)\n",
        "        correct += pred.eq(y.view_as(pred)).sum().item()\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / num_batches\n",
        "    avg_acc = correct / size\n",
        "\n",
        "    return avg_epoch_loss, avg_acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    num_batches = len(dataloader)\n",
        "    size = len(dataloader.dataset)\n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in tqdm(dataloader):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            pred = model(X)\n",
        "\n",
        "            epoch_loss += loss_fn(pred, y).item()\n",
        "            pred = pred.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(y.view_as(pred)).sum().item()\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / num_batches\n",
        "    avg_acc = correct / size\n",
        "\n",
        "    return avg_epoch_loss, avg_acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = 50\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_loss_list = []\n",
        "test_acc_list = []\n",
        "best_acc = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss, train_acc = train(train_dataloader, model, loss_fn, optimizer)\n",
        "    # train_loss, train_acc = train(\n",
        "    #     sixteenth_train_dataloader, model, loss_fn, optimizer)\n",
        "    # train_loss, train_acc = train(\n",
        "    #     half_train_dataloader, model, loss_fn, optimizer)\n",
        "\n",
        "    test_loss, test_acc = test(valid_dataloader, model, loss_fn)\n",
        "\n",
        "    if test_acc > best_acc:\n",
        "        best_acc = test_acc\n",
        "        # torch.save(model.state_dict(), 'resnet18_all_IMAGENET.pth')\n",
        "        # torch.save(model.state_dict(), 'resnet18_sixteenth_IMAGENET.pth')\n",
        "        # torch.save(model.state_dict(), 'resnet18_half_IMAGENET.pth')\n",
        "        torch.save(model.state_dict(), 'resnet50_all_IMAGENET.pth')         \n",
        "        # torch.save(model.state_dict(), 'resnet50_sixteenth_IMAGENET.pth')         \n",
        "        # torch.save(model.state_dict(), 'resnet50_half_IMAGENET.pth')\n",
        "\n",
        "    train_loss_list.append(train_loss)\n",
        "    train_acc_list.append(train_acc)\n",
        "    test_loss_list.append(test_loss)\n",
        "    test_acc_list.append(test_acc)\n",
        "    \n",
        "    print(f\"Epoch {epoch + 1:2d}: Loss = {train_loss:.4f} Acc = {train_acc:.2f} Test_Loss = {test_loss:.4f} Test_Acc = {test_acc:.2f}\")\n",
        "print(\"Done!\")\n",
        "print(f\"Best Accuracy: {best_acc:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_loss_list, label='train_loss') \n",
        "plt.plot(test_loss_list, label='test_loss') \n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_acc_list, label='train_acc')\n",
        "plt.plot(test_acc_list, label='test_acc')\n",
        "plt.legend() \n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot the relationship between Dataset Size and Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resnet50_test_acc = []\n",
        "resnet18_test_acc = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', weights=None)\n",
        "model.fc = nn.Linear(model.fc.in_features, 10)\n",
        "model = model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load('resnet50_sixteenth_IMAGENET.pth'))\n",
        "model.eval()\n",
        "test_loss, test_acc = test(valid_dataloader, model, loss_fn)\n",
        "print(f\"Test Loss = {test_loss:.4f} Test Acc = {test_acc:.2f}\")\n",
        "resnet50_test_acc.append(test_acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load('resnet50_half_IMAGENET.pth'))\n",
        "model.eval()\n",
        "test_loss, test_acc = test(valid_dataloader, model, loss_fn)\n",
        "print(f\"Test Loss = {test_loss:.4f} Test Acc = {test_acc:.2f}\")\n",
        "resnet50_test_acc.append(test_acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load('resnet50_all_IMAGENET.pth'))\n",
        "model.eval()\n",
        "test_loss, test_acc = test(valid_dataloader, model, loss_fn)\n",
        "print(f\"Test Loss = {test_loss:.4f} Test Acc = {test_acc:.2f}\")\n",
        "resnet50_test_acc.append(test_acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', weights=None)\n",
        "model.fc = nn.Linear(model.fc.in_features, 10)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load('resnet18_sixteenth_IMAGENET.pth'))\n",
        "model.eval()\n",
        "test_loss, test_acc = test(valid_dataloader, model, loss_fn)\n",
        "print(f\"Test Loss = {test_loss:.4f} Test Acc = {test_acc:.2f}\")\n",
        "resnet18_test_acc.append(test_acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load('resnet18_half_IMAGENET.pth'))\n",
        "model.eval()\n",
        "test_loss, test_acc = test(valid_dataloader, model, loss_fn)\n",
        "print(f\"Test Loss = {test_loss:.4f} Test Acc = {test_acc:.2f}\")\n",
        "resnet18_test_acc.append(test_acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load('resnet18_all_IMAGENET.pth'))\n",
        "model.eval()\n",
        "test_loss, test_acc = test(valid_dataloader, model, loss_fn)\n",
        "print(f\"Test Loss = {test_loss:.4f} Test Acc = {test_acc:.2f}\")\n",
        "resnet18_test_acc.append(test_acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Dataset_size = ['Sixteenth', 'Half', 'All']\n",
        "resnet50 = resnet50_test_acc\n",
        "resnet18 = resnet18_test_acc\n",
        "# Dataset_size is axis x and Test_Accuracy is axis y\n",
        "plt.plot(Dataset_size, resnet50, label='resnet50_IMAGENET')\n",
        "plt.plot(Dataset_size, resnet18, label='resnet18_IMAGENET')\n",
        "plt.legend()\n",
        "plt.xlabel('Dataset Size')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iqBGAUm6b5W"
      },
      "source": [
        "## Codes for Problem 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SBFMzPT6cP4"
      },
      "outputs": [],
      "source": [
        "# TODO: Try to achieve the best performance given all training data using whatever model and training strategy.\n",
        "import torchvision.models\n",
        "model = models.convnext_base(weights=models.ConvNeXt_Base_Weights.IMAGENET1K_V1)\n",
        "model.classifier[2] = nn.Linear(model.classifier[2].in_features, 10)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Pad(4, padding_mode='reflect'),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VHp_O3_JgZE"
      },
      "outputs": [],
      "source": [
        "batch_size = 256\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='data', train=True, download=True, transform=train_transform)\n",
        "valid_dataset = datasets.CIFAR10(root='data', train=False, download=True, transform=test_transform)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlXKJeYWnFTA"
      },
      "outputs": [],
      "source": [
        "# TODO: Fill in the code cell according to the pytorch tutorial we gave.\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = 100\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_loss_list = []\n",
        "test_acc_list = []\n",
        "best_acc = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    num_batches = len(dataloader)\n",
        "    size = len(dataloader.dataset)\n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for X, y in tqdm(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        pred = pred.argmax(dim=1, keepdim=True)\n",
        "        correct += pred.eq(y.view_as(pred)).sum().item()\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / num_batches\n",
        "    avg_acc = correct / size\n",
        "\n",
        "    return avg_epoch_loss, avg_acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    num_batches = len(dataloader)\n",
        "    size = len(dataloader.dataset)\n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in tqdm(dataloader):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            pred = model(X)\n",
        "\n",
        "            epoch_loss += loss_fn(pred, y).item()\n",
        "            pred = pred.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(y.view_as(pred)).sum().item()\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / num_batches\n",
        "    avg_acc = correct / size\n",
        "\n",
        "    return avg_epoch_loss, avg_acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for epoch in range(epochs):\n",
        "    train_loss, train_acc = train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loss, test_acc = test(valid_dataloader, model, loss_fn)\n",
        "    scheduler.step()\n",
        "\n",
        "    if test_acc > best_acc:\n",
        "        best_acc = test_acc\n",
        "        torch.save(model.state_dict(), 'convnext_base.pth')\n",
        "\n",
        "    train_loss_list.append(train_loss)\n",
        "    train_acc_list.append(train_acc)\n",
        "    test_loss_list.append(test_loss)\n",
        "    test_acc_list.append(test_acc)\n",
        "    \n",
        "    print(f\"Epoch {epoch + 1:2d}: Loss = {train_loss:.4f} Acc = {train_acc:.2f} Test_Loss = {test_loss:.4f} Test_Acc = {test_acc:.2f}\")\n",
        "print(\"Done!\")\n",
        "print(f\"Best Accuracy: {best_acc:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_loss_list, label='train_loss') \n",
        "plt.plot(test_loss_list, label='test_loss') \n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_acc_list, label='train_acc')\n",
        "plt.plot(test_acc_list, label='test_acc')\n",
        "plt.legend() \n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTSafuelpRYJ"
      },
      "source": [
        "## Problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZctBdkurpQS"
      },
      "source": [
        "1. (30%) Finish the rest of the codes for Problem 1 and Problem 2 according to the hint. (2 code cells in total.)\n",
        "2. Train small model (resnet18) and big model (resnet50) from scratch on `sixteenth_train_dataloader`, `half_train_dataloader`, and `train_dataloader` respectively.\n",
        "3. (30%) Achieve the best performance given all training data using whatever model and training strategy.  \n",
        "  (You cannot use the model that was pretrained on CIFAR10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "786fQTdk0msC"
      },
      "source": [
        "## Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsd2yTmB0k5t"
      },
      "source": [
        "\n",
        "- (30%) The relationship between the accuracy, model size, and the training dataset size.  \n",
        "    (Total 6 models. Small model trains on the sixteenth, half, and all data. Big model trains on the sixteenth, half, and all data.)\n",
        "- (10%) What if we train the ResNet with ImageNet initialized weights (`weights=\"IMAGENET1K_V1\"`), how would the relationship change?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWDxF-xIueMM"
      },
      "source": [
        "## Credits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sXpmSj2ufkh"
      },
      "source": [
        "1. [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MemcOLK_4ULJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.6 ('pytorch')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "01e90ddb7d2249497434e58949539ea6100e0e184c6ee6c37c47644934366b4f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
